## **The Research for Community-Oriented Lexical Simplification** 

lexical simplification aims at converting complex words into simplified words with the same semantic meaning in text, which would however be understood better by reader. This task would potentially benefit many applications such as assisting in reading for groups with cognitive disabilities and low education. It can also be used to help non-professional groups to understand complex term in professional fields such as law and medicine. Therefore, lexical simplification has great value of application and social significance. Lexical simplification would also be useful as a pre-processing tool for other Natural Language Processing (NLP) tasks, such as improve the readability of the output results of machine translation and summarization.

### **Pre-trained model**

- [BERT](https://github.com/qiang2100/BERT-LS)
-  [Context2vec](https://github.com/orenmel/context2vec)

### **How to run this code**

 (1) Download Word2vec word embeddings.

```
run python2 /Context-aware/Tony_WordNet_similarity.py
```

- This will create the a <result-file>
- Run the following to compute the candidate ranking  score

```
run python2 /Context-aware/Tony_Method_Context_ngrams_testing.py 
```

Context-aware method detailed reference papers published in [Springer, Cham](https://doi.org/10.1007/978-3-030-59413-8_7)

(2) Download the word embeddings, context embeddings from [[here](http://u.cs.biu.ac.il/~nlp/resources/downloads/lexsub_embeddings/)].

- Unzip a model into MODEL_DIR
- Run:

```
python3 /Context2vec/context2vec-master/Context2vec/eval/explore_context2vec.py MODEL_DIR/MODEL_NAME.params
```

**You can train a new context2vec model**

- CORPUS_FILE needs to contain your learning corpus with one sentence per line and tokens separated by spaces.
- Run:

```
python3 /Context2vec/context2vec-master/context2vec//train/corpus_by_sent_length.py CORPUS_FILE [max-sentence-length]
```

- This will create a directory CORPUS_FILE.DIR that will contain your preprocessed learning corpus
- Run:

```
python3 context2vec//train/train_context2vec.py -i CORPUS_FILE.DIR  -w  WORD_EMBEDDINGS -m MODEL  -c lstm --deep yes -t 3 --dropout 0.0 -u 300 -e 10 -p 0.75 -b 100 -g 0
```

- This will create WORD_EMBEDDINGS.targets file with your target word embeddings, a MODEL file, and a MODEL.params file. Put all of these in the same directory MODEL_DIR and you're done.

Merging candidate words generated by  Context2vec neural model and  Context-aware model based on a weighted average method and Boosting method. 

- Run:

```
python2 /Context2vec/context2vec-master/context2vec//train_weight.py
```

Context2vec method detailed reference papers published in [Springer, Cham](https://doi.org/10.1007/978-3-030-60450-9_11)

(3) Download the Download pretrianed BERT. In our experiments, we adopted pretrained [BERT-Large, Uncased (Whole Word Masking)](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip).

(4) Download the pre-trained word embeddings using [FastText](https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip).

(5) Download an English paraphrase database ([PPDB](http://paraphrase.org/#/download)) , and assign the path of PPDB in the ".sh" file.

- Run:

```
"./run_LSBert2.sh"
```

Context-aware method detailed reference papers published in [AAAI2020](https://arxiv.org/abs/1907.06226) and [arXiv](https://arxiv.org/abs/2006.14939)

Merging candidate words generated by  BERT model and  Context-aware model based on a weighted average method and Boosting method. 

- Run:

```
python2 /Context2vec/context2vec-master/context2vec//train_weight.py
```

Merging candidate words generated by  BERT model and  Context2vec model based on a weighted average method and Boosting method. 

- Run:

```
python2 /Context2vec/context2vec-master/context2vec//train_weight.py
```

### **Idea**

(1) In order to solve the low ratio problem of candidate words generation by the Context-aware model and to improve the generation accuracy of candidate words of the Context2vec model, we try to use a model merging strategy as a weighted average method and a boosting method. 

(2) When the generation rate of candidate words is high enough, we assume that if a candidate word appears in multiple lexical simplification models, we enhance the weight of the candidate word.

